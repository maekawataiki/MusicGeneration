{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music_Style_Transfer_Cycle_GAN_inference_only.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO3tme2CH7q4GlKc4DpSOsK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maekawataiki/MusicGeneration/blob/master/Style_Transformer_Cycle_GAN/Music_Style_Transfer_Cycle_GAN_inference_only.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrYTwtXqunHw",
        "colab_type": "text"
      },
      "source": [
        "# Music Style Transfer with Cycle GAN\n",
        "\n",
        "This notebook demonstrates the trainining and inference of Cycle-GAN.\n",
        "\n",
        "This note book is for inference only. If you want to train the model on your dataset, there is [another notebook]().\n",
        "\n",
        "Based on paper\n",
        "[Symbolic Music Genre Transfer with CycleGAN](https://arxiv.org/pdf/1809.07575.pdf)\n",
        "\n",
        "## Changes from original paper\n",
        "\n",
        "- Identity loss is added to Generator loss which significantly improved convergence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPvCfVXdufuA",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Clone repository\n",
        "\n",
        "!git clone https://github.com/sumuzhao/CycleGAN-Music-Style-Transfer-Refactorization\n",
        "%cd CycleGAN-Music-Style-Transfer-Refactorization\n",
        "!mkdir models\n",
        "!pip install pretty_midi pypianoroll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcZmjOg629A8",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Select model type\n",
        "\n",
        "model_type = 'Classic <-> Pop' #@param [\"Classic <-> Pop\", \"Jazz <-> Pop\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnT3zTIAu4vM",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Download pre-trained model\n",
        "\n",
        "if model_type == 'Classic <-> Pop':\n",
        "  !gdown https://drive.google.com/uc?id=10Q2v1Fad0kvdc_fB2ZDritwa7xA1kQ3Y\n",
        "  !unzip CP_C2CP_P_2020-08-31_base_0.1.zip -d ./models\n",
        "else:\n",
        "  !gdown https://drive.google.com/uc?id=124-o9uchvYJOw5wgoefdNeomhBKymUTh\n",
        "  !unzip JP_J2JP_P_2020-08-28_base_0.01.zip -d ./models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1ytO9P90ZAx",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Import Dependencies\n",
        "\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import copy\n",
        "from glob import glob\n",
        "from collections import namedtuple\n",
        "\n",
        "import pretty_midi\n",
        "from pypianoroll import Multitrack, Track\n",
        "\n",
        "from google.colab import files, drive\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, layers, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import write_midi\n",
        "from tf2_module import abs_criterion, mae_criterion\n",
        "from tf2_utils import get_now_datetime, ImagePool, to_binary, load_npy_data, save_midis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIYjeC8kyO85",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Define Model\n",
        "\n",
        "def get_bar_piano_roll(piano_roll):\n",
        "    if int(piano_roll.shape[0] % 64) is not 0:\n",
        "        if LAST_BAR_MODE == 'fill':\n",
        "            piano_roll = np.concatenate((piano_roll, np.zeros((64 - piano_roll.shape[0] % 64, 128))), axis=0)\n",
        "        elif LAST_BAR_MODE == 'remove':\n",
        "            piano_roll = np.delete(piano_roll,  np.s_[-int(piano_roll.shape[0] % 64):], axis=0)\n",
        "    piano_roll = piano_roll.reshape(-1, 64, 128)\n",
        "    return piano_roll\n",
        "\n",
        "def padding(x, p=3):\n",
        "    return tf.pad(x, [[0, 0], [p, p], [p, p], [0, 0]], \"REFLECT\")\n",
        "\n",
        "class InstanceNorm(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(InstanceNorm, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.scale = self.add_weight(name='SCALE',\n",
        "                                     shape=input_shape[-1:],\n",
        "                                     initializer=tf.keras.initializers.random_normal(1., 0.02),\n",
        "                                     trainable=True,\n",
        "                                     dtype=tf.float32)\n",
        "        self.offset = self.add_weight(name='OFFSET',\n",
        "                                      shape=input_shape[-1:],\n",
        "                                      initializer=tf.keras.initializers.zeros(),\n",
        "                                      trainable=True,\n",
        "                                      dtype=tf.float32)\n",
        "\n",
        "    def call(self, x, epsilon=1e-5):\n",
        "          mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n",
        "          inv = tf.math.rsqrt(variance + epsilon)\n",
        "          normalized = (x - mean) * inv\n",
        "          return self.scale * normalized + self.offset\n",
        "\n",
        "def build_resnet_block(dim, k_init, ks=3, s=1, name='Resnet'):\n",
        "\n",
        "    inputs = Input(shape=(128, 128, 256))\n",
        "    x = inputs\n",
        "\n",
        "    # e.g, x is (batch * 128 * 128 * 3)\n",
        "    p = (ks - 1) // 2\n",
        "    # For ks = 3, p = 1\n",
        "    y = layers.Lambda(padding,\n",
        "                      arguments={'p': p},\n",
        "                      name='PADDING_1')(x)\n",
        "    # After first padding, (batch * 130 * 130 * 3)\n",
        "\n",
        "    y = layers.Conv2D(filters=dim,\n",
        "                      kernel_size=ks,\n",
        "                      strides=s,\n",
        "                      padding='valid',\n",
        "                      kernel_initializer=k_init,\n",
        "                      use_bias=False)(y)\n",
        "    y = InstanceNorm(name='IN_1')(y)\n",
        "    y = layers.ReLU()(y)\n",
        "    # After first conv2d, (batch * 128 * 128 * 3)\n",
        "\n",
        "    y = layers.Lambda(padding,\n",
        "                      arguments={'p': p},\n",
        "                      name='PADDING_2')(y)\n",
        "    # After second padding, (batch * 130 * 130 * 3)\n",
        "\n",
        "    y = layers.Conv2D(filters=dim,\n",
        "                      kernel_size=ks,\n",
        "                      strides=s,\n",
        "                      padding='valid',\n",
        "                      kernel_initializer=k_init,\n",
        "                      use_bias=False)(y)\n",
        "    y = InstanceNorm(name='IN_2')(y)\n",
        "    y = layers.ReLU()(y + x)\n",
        "    # After second conv2d, (batch * 128 * 128 * 3)\n",
        "    outputs = y\n",
        "    return Model(inputs=inputs,\n",
        "                outputs=outputs,\n",
        "                name=name)\n",
        "\n",
        "def build_discriminator(options, name='Discriminator'):\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    inputs = Input(shape=(options.time_step,\n",
        "                          options.pitch_range,\n",
        "                          options.output_nc))\n",
        "\n",
        "    x = inputs\n",
        "\n",
        "    x = layers.Conv2D(filters=options.df_dim,\n",
        "                      kernel_size=7,\n",
        "                      strides=2,\n",
        "                      padding='same',\n",
        "                      kernel_initializer=initializer,\n",
        "                      use_bias=False,\n",
        "                      name='CONV2D_1')(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    # (batch * 32 * 42 * 64)\n",
        "\n",
        "    x = layers.Conv2D(filters=options.df_dim * 4,\n",
        "                      kernel_size=7,\n",
        "                      strides=2,\n",
        "                      padding='same',\n",
        "                      kernel_initializer=initializer,\n",
        "                      use_bias=False,\n",
        "                      name='CONV2D_2')(x)\n",
        "    x = InstanceNorm(name='IN_1')(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    # (batch * 16 * 21 * 256)\n",
        "\n",
        "    x = layers.Conv2D(filters=1,\n",
        "                      kernel_size=7,\n",
        "                      strides=1,\n",
        "                      padding='same',\n",
        "                      kernel_initializer=initializer,\n",
        "                      use_bias=False,\n",
        "                      name='CONV2D_3')(x)\n",
        "    # (batch * 16 * 21 * 1)\n",
        "\n",
        "    outputs = x\n",
        "\n",
        "    return Model(inputs=inputs,\n",
        "                 outputs=outputs,\n",
        "                 name=name)\n",
        "\n",
        "\n",
        "def build_generator(options, name='Generator'):\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    inputs = Input(shape=(options.time_step,\n",
        "                          options.pitch_range,\n",
        "                          options.output_nc))\n",
        "\n",
        "    x = inputs\n",
        "    # (batch * 64 * 84 * 1)\n",
        "\n",
        "    x = layers.Lambda(padding,\n",
        "                      name='PADDING_1')(x)\n",
        "    # (batch * 70 * 90 * 1)\n",
        "\n",
        "    x = layers.Conv2D(filters=options.gf_dim,\n",
        "                      kernel_size=7,\n",
        "                      strides=1,\n",
        "                      padding='valid',\n",
        "                      kernel_initializer=initializer,\n",
        "                      use_bias=False,\n",
        "                      name='CONV2D_1')(x)\n",
        "    x = InstanceNorm(name='IN_1')(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    # (batch * 64 * 84 * 64)\n",
        "\n",
        "    x = layers.Conv2D(filters=options.gf_dim * 2,\n",
        "                      kernel_size=3,\n",
        "                      strides=2,\n",
        "                      padding='same',\n",
        "                      kernel_initializer=initializer,\n",
        "                      use_bias=False,\n",
        "                      name='CONV2D_2')(x)\n",
        "    x = InstanceNorm(name='IN_2')(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    # (batch * 32 * 42 * 128)\n",
        "\n",
        "    x = layers.Conv2D(filters=options.gf_dim * 4,\n",
        "                      kernel_size=3,\n",
        "                      strides=2,\n",
        "                      padding='same',\n",
        "                      kernel_initializer=initializer,\n",
        "                      use_bias=False,\n",
        "                      name='CONV2D_3')(x)\n",
        "    x = InstanceNorm(name='IN_3')(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    # (batch * 16 * 21 * 256)\n",
        "\n",
        "    for i in range(10):\n",
        "        x = build_resnet_block(dim=options.gf_dim * 4, \n",
        "                               k_init=initializer,\n",
        "                               name='ResNet_Block_{}'.format(i))(x)\n",
        "    # (batch * 16 * 21 * 256)\n",
        "\n",
        "    x = layers.Conv2DTranspose(filters=options.gf_dim * 2,\n",
        "                               kernel_size=3,\n",
        "                               strides=2,\n",
        "                               padding='same',\n",
        "                               kernel_initializer=initializer,\n",
        "                               use_bias=False,\n",
        "                               name='DECONV2D_1')(x)\n",
        "    x = InstanceNorm(name='IN_4')(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    # (batch * 32 * 42 * 128)\n",
        "\n",
        "    x = layers.Conv2DTranspose(filters=options.gf_dim,\n",
        "                               kernel_size=3,\n",
        "                               strides=2,\n",
        "                               padding='same',\n",
        "                               kernel_initializer=initializer,\n",
        "                               use_bias=False,\n",
        "                               name='DECONV2D_2')(x)\n",
        "    x = InstanceNorm(name='IN_5')(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    # (batch * 64 * 84 * 64)\n",
        "\n",
        "    x = layers.Lambda(padding,\n",
        "                      name='PADDING_2')(x)\n",
        "    # After padding, (batch * 70 * 90 * 64)\n",
        "\n",
        "    x = layers.Conv2D(filters=options.output_nc,\n",
        "                      kernel_size=7,\n",
        "                      strides=1,\n",
        "                      padding='valid',\n",
        "                      kernel_initializer=initializer,\n",
        "                      activation='sigmoid',\n",
        "                      use_bias=False,\n",
        "                      name='CONV2D_4')(x)\n",
        "    # (batch * 64 * 84 * 1)\n",
        "\n",
        "    outputs = x\n",
        "\n",
        "    return Model(inputs=inputs,\n",
        "                 outputs=outputs,\n",
        "                 name=name)\n",
        "\n",
        "\n",
        "def build_discriminator_classifier(options, name='Discriminator_Classifier'):\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    inputs = Input(shape=(options.time_step,\n",
        "                          options.pitch_range,\n",
        "                          options.output_nc))\n",
        "\n",
        "    x = inputs\n",
        "    # (batch * 64, 84, 1)\n",
        "\n",
        "    x = layers.Conv2D(filters=options.df_dim,\n",
        "                      kernel_size=[1, 12],\n",
        "                      strides=[1, 12],\n",
        "                      padding='same',\n",
        "                      kernel_initializer=initializer,\n",
        "                      use_bias=False,\n",
        "                      name='CONV2D_1')(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    # (batch * 64 * 7 * 64)\n",
        "\n",
        "    x = layers.Conv2D(filters=options.df_dim * 2,\n",
        "                      kernel_size=[4, 1],\n",
        "                      strides=[4, 1],\n",
        "                      padding='same',\n",
        "                      kernel_initializer=initializer,\n",
        "                      use_bias=False,\n",
        "                      name='CONV2D_2')(x)\n",
        "    x = InstanceNorm(name='IN_1')(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    # (batch * 16 * 7 * 128)\n",
        "\n",
        "    x = layers.Conv2D(filters=options.df_dim * 4,\n",
        "                      kernel_size=[2, 1],\n",
        "                      strides=[2, 1],\n",
        "                      padding='same',\n",
        "                      kernel_initializer=initializer,\n",
        "                      use_bias=False,\n",
        "                      name='CONV2D_3')(x)\n",
        "    x = InstanceNorm(name='IN_2')(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    # (batch * 8 * 7 * 256)\n",
        "\n",
        "    x = layers.Conv2D(filters=options.df_dim * 8,\n",
        "                      kernel_size=[8, 1],\n",
        "                      strides=[8, 1],\n",
        "                      padding='same',\n",
        "                      kernel_initializer=initializer,\n",
        "                      use_bias=False,\n",
        "                      name='CONV2D_4')(x)\n",
        "    x = InstanceNorm(name='IN_3')(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    # (batch * 1 * 7 * 512)\n",
        "\n",
        "    x = layers.Conv2D(filters=2,\n",
        "                      kernel_size=[1, 7],\n",
        "                      strides=[1, 7],\n",
        "                      padding='same',\n",
        "                      kernel_initializer=initializer,\n",
        "                      use_bias=False,\n",
        "                      name='CONV2D_5')(x)\n",
        "    # (batch * 1 * 1 * 2)\n",
        "\n",
        "    x = tf.reshape(x, [-1, 2])\n",
        "    # (batch * 2)\n",
        "\n",
        "    outputs = x\n",
        "\n",
        "    return Model(inputs=inputs,\n",
        "                 outputs=outputs,\n",
        "                 name=name)\n",
        "\n",
        "class CycleGAN(object):\n",
        "\n",
        "    def __init__(self, args):\n",
        "\n",
        "        self.batch_size = args.batch_size\n",
        "        self.time_step = args.time_step  # number of time steps\n",
        "        self.pitch_range = args.pitch_range  # number of pitches\n",
        "        self.input_c_dim = args.input_nc  # number of input image channels\n",
        "        self.output_c_dim = args.output_nc  # number of output image channels\n",
        "        self.lr = args.lr\n",
        "        self.L1_lambda = args.L1_lambda\n",
        "        self.gamma = args.gamma\n",
        "        self.sigma_d = args.sigma_d\n",
        "        self.dataset_A_dir = args.dataset_A_dir\n",
        "        self.dataset_B_dir = args.dataset_B_dir\n",
        "        self.sample_dir = args.sample_dir\n",
        "\n",
        "        self.model = args.model\n",
        "        self.discriminator = build_discriminator\n",
        "        self.generator = build_generator\n",
        "        self.criterionGAN = mae_criterion\n",
        "\n",
        "        OPTIONS = namedtuple('OPTIONS', 'batch_size '\n",
        "                                        'time_step '\n",
        "                                        'input_nc '\n",
        "                                        'output_nc '\n",
        "                                        'pitch_range '\n",
        "                                        'gf_dim '\n",
        "                                        'df_dim '\n",
        "                                        'is_training')\n",
        "        self.options = OPTIONS._make((args.batch_size,\n",
        "                                      args.time_step,\n",
        "                                      args.pitch_range,\n",
        "                                      args.input_nc,\n",
        "                                      args.output_nc,\n",
        "                                      args.ngf,\n",
        "                                      args.ndf,\n",
        "                                      args.phase == 'train'))\n",
        "\n",
        "        self.now_datetime = get_now_datetime()\n",
        "        self.pool = ImagePool(args.max_size)\n",
        "\n",
        "        self._build_model(args)\n",
        "\n",
        "        print(\"initialize model...\")\n",
        "\n",
        "    def _build_model(self, args):\n",
        "\n",
        "        # Generator\n",
        "        self.generator_A2B = self.generator(self.options,\n",
        "                                            name='Generator_A2B')\n",
        "        self.generator_B2A = self.generator(self.options,\n",
        "                                            name='Generator_B2A')\n",
        "\n",
        "        # Discriminator\n",
        "        self.discriminator_A = self.discriminator(self.options,\n",
        "                                                  name='Discriminator_A')\n",
        "        self.discriminator_B = self.discriminator(self.options,\n",
        "                                                  name='Discriminator_B')\n",
        "\n",
        "        if self.model != 'base':\n",
        "            self.discriminator_A_all = self.discriminator(self.options,\n",
        "                                                          name='Discriminator_A_all')\n",
        "            self.discriminator_B_all = self.discriminator(self.options,\n",
        "                                                          name='Discriminator_B_all')\n",
        "\n",
        "        # Discriminator and Generator Optimizer\n",
        "        self.DA_optimizer = Adam(self.lr,\n",
        "                                 beta_1=args.beta1)\n",
        "        self.DB_optimizer = Adam(self.lr,\n",
        "                                 beta_1=args.beta1)\n",
        "        self.GA2B_optimizer = Adam(self.lr,\n",
        "                                   beta_1=args.beta1)\n",
        "        self.GB2A_optimizer = Adam(self.lr,\n",
        "                                   beta_1=args.beta1)\n",
        "\n",
        "        if self.model != 'base':\n",
        "            self.DA_all_optimizer = Adam(self.lr,\n",
        "                                         beta_1=args.beta1)\n",
        "            self.DB_all_optimizer = Adam(self.lr,\n",
        "                                         beta_1=args.beta1)\n",
        "\n",
        "        # Checkpoints\n",
        "        model_name = \"cyclegan.model\"\n",
        "        model_dir = \"{}2{}_{}_{}_{}\".format(self.dataset_A_dir,\n",
        "                                            self.dataset_B_dir,\n",
        "                                            self.now_datetime,\n",
        "                                            self.model,\n",
        "                                            self.sigma_d)\n",
        "        self.checkpoint_dir = os.path.join(args.checkpoint_dir,\n",
        "                                           args.model_dir or model_dir,\n",
        "                                           model_name)\n",
        "        if not os.path.exists(self.checkpoint_dir):\n",
        "            os.makedirs(self.checkpoint_dir)\n",
        "\n",
        "        if self.model == 'base':\n",
        "            self.checkpoint = tf.train.Checkpoint(generator_A2B_optimizer=self.GA2B_optimizer,\n",
        "                                                  generator_B2A_optimizer=self.GB2A_optimizer,\n",
        "                                                  discriminator_A_optimizer=self.DA_optimizer,\n",
        "                                                  discriminator_B_optimizer=self.DB_optimizer,\n",
        "                                                  generator_A2B=self.generator_A2B,\n",
        "                                                  generator_B2A=self.generator_B2A,\n",
        "                                                  discriminator_A=self.discriminator_A,\n",
        "                                                  discriminator_B=self.discriminator_B)\n",
        "        else:\n",
        "            self.checkpoint = tf.train.Checkpoint(generator_A2B_optimizer=self.GA2B_optimizer,\n",
        "                                                  generator_B2A_optimizer=self.GB2A_optimizer,\n",
        "                                                  discriminator_A_optimizer=self.DA_optimizer,\n",
        "                                                  discriminator_B_optimizer=self.DB_optimizer,\n",
        "                                                  discriminator_A_all_optimizer=self.DA_all_optimizer,\n",
        "                                                  discriminator_B_all_optimizer=self.DB_all_optimizer,\n",
        "                                                  generator_A2B=self.generator_A2B,\n",
        "                                                  generator_B2A=self.generator_B2A,\n",
        "                                                  discriminator_A=self.discriminator_A,\n",
        "                                                  discriminator_B=self.discriminator_B,\n",
        "                                                  discriminator_A_all=self.discriminator_A_all,\n",
        "                                                  discriminator_B_all=self.discriminator_B_all)\n",
        "\n",
        "        self.checkpoint_manager = tf.train.CheckpointManager(self.checkpoint,\n",
        "                                                             self.checkpoint_dir,\n",
        "                                                             max_to_keep=5)\n",
        "\n",
        "        # if self.checkpoint_manager.latest_checkpoint:\n",
        "        #     self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint)\n",
        "        #     print('Latest checkpoint restored!!')\n",
        "\n",
        "    def train(self, args):\n",
        "\n",
        "        # Data from domain A and B, and mixed dataset for partial and full models.\n",
        "        dataA = glob('./datasets/{}/train/*.*'.format(self.dataset_A_dir))\n",
        "        dataB = glob('./datasets/{}/train/*.*'.format(self.dataset_B_dir))\n",
        "        data_mixed = None\n",
        "        if self.model == 'partial':\n",
        "            data_mixed = dataA + dataB\n",
        "        if self.model == 'full':\n",
        "            data_mixed = glob('./datasets/JCP_mixed/*.*')\n",
        "\n",
        "        if args.continue_train:\n",
        "            if self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint):\n",
        "                print(\" [*] Load checkpoint succeeded!\")\n",
        "            else:\n",
        "                print(\" [!] Load checkpoint failed...\")\n",
        "\n",
        "        counter = 1\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(args.epoch):\n",
        "\n",
        "            # Shuffle training data\n",
        "            np.random.shuffle(dataA)\n",
        "            np.random.shuffle(dataB)\n",
        "            if self.model != 'base' and data_mixed is not None:\n",
        "                np.random.shuffle(data_mixed)\n",
        "\n",
        "            # Get the proper number of batches\n",
        "            batch_idxs = min(len(dataA), len(dataB)) // self.batch_size\n",
        "\n",
        "            # learning rate starts to decay when reaching the threshold\n",
        "            self.lr = self.lr if epoch < args.epoch_step else self.lr * (args.epoch-epoch) / (args.epoch-args.epoch_step)\n",
        "\n",
        "            for idx in range(batch_idxs):\n",
        "\n",
        "                # To feed real_data\n",
        "                batch_files = list(zip(dataA[idx * self.batch_size:(idx + 1) * self.batch_size],\n",
        "                                       dataB[idx * self.batch_size:(idx + 1) * self.batch_size]))\n",
        "                batch_samples = [load_npy_data(batch_file) for batch_file in batch_files]\n",
        "                batch_samples = np.array(batch_samples).astype(np.float32)  # batch_size * 64 * 84 * 2\n",
        "                real_A, real_B = batch_samples[:, :, :, 0], batch_samples[:, :, :, 1]\n",
        "                real_A = tf.expand_dims(real_A, -1)\n",
        "                real_B = tf.expand_dims(real_B, -1)\n",
        "\n",
        "                # generate gaussian noise for robustness improvement\n",
        "                gaussian_noise = np.abs(np.random.normal(0,\n",
        "                                                         self.sigma_d,\n",
        "                                                         [self.batch_size,\n",
        "                                                          self.time_step,\n",
        "                                                          self.pitch_range,\n",
        "                                                          self.input_c_dim])).astype(np.float32)\n",
        "\n",
        "                if self.model == 'base':\n",
        "\n",
        "                    with tf.GradientTape(persistent=True) as gen_tape, tf.GradientTape(persistent=True) as disc_tape:\n",
        "\n",
        "                        fake_B = self.generator_A2B(real_A,\n",
        "                                                    training=True)\n",
        "                        cycle_A = self.generator_B2A(fake_B,\n",
        "                                                     training=True)\n",
        "\n",
        "                        fake_A = self.generator_B2A(real_B,\n",
        "                                                    training=True)\n",
        "                        cycle_B = self.generator_A2B(fake_A,\n",
        "                                                     training=True)\n",
        "                        \n",
        "                        # Added for identity loss\n",
        "                        same_B = self.generator_A2B(real_B,\n",
        "                                                    training=True)\n",
        "                        same_A = self.generator_B2A(real_A,\n",
        "                                                    training=True)\n",
        "\n",
        "                        [fake_A_sample, fake_B_sample] = self.pool([fake_A, fake_B])\n",
        "\n",
        "                        DA_real = self.discriminator_A(real_A + gaussian_noise,\n",
        "                                                       training=True)\n",
        "                        DB_real = self.discriminator_B(real_B + gaussian_noise,\n",
        "                                                       training=True)\n",
        "\n",
        "                        DA_fake = self.discriminator_A(fake_A + gaussian_noise,\n",
        "                                                       training=True)\n",
        "                        DB_fake = self.discriminator_B(fake_B + gaussian_noise,\n",
        "                                                       training=True)\n",
        "\n",
        "                        DA_fake_sample = self.discriminator_A(fake_A_sample + gaussian_noise,\n",
        "                                                              training=True)\n",
        "                        DB_fake_sample = self.discriminator_B(fake_B_sample + gaussian_noise,\n",
        "                                                              training=True)\n",
        "\n",
        "                        # Generator loss\n",
        "                        cycle_loss = self.L1_lambda * (abs_criterion(real_A, cycle_A) + abs_criterion(real_B, cycle_B))\n",
        "                        identity_loss_A = self.L1_lambda * 0.5 * abs_criterion(real_A, same_A) # added\n",
        "                        identity_loss_B = self.L1_lambda * 0.5 * abs_criterion(real_B, same_B) # added\n",
        "                        g_A2B_loss = self.criterionGAN(DB_fake, tf.ones_like(DB_fake)) + cycle_loss + identity_loss_B\n",
        "                        g_B2A_loss = self.criterionGAN(DA_fake, tf.ones_like(DA_fake)) + cycle_loss + identity_loss_A\n",
        "                        g_loss = g_A2B_loss + g_B2A_loss - cycle_loss\n",
        "\n",
        "                        # Discriminator loss\n",
        "                        d_A_loss_real = self.criterionGAN(DA_real, tf.ones_like(DA_real))\n",
        "                        d_A_loss_fake = self.criterionGAN(DA_fake_sample, tf.zeros_like(DA_fake_sample))\n",
        "                        d_A_loss = (d_A_loss_real + d_A_loss_fake) / 2\n",
        "                        d_B_loss_real = self.criterionGAN(DB_real, tf.ones_like(DB_real))\n",
        "                        d_B_loss_fake = self.criterionGAN(DB_fake_sample, tf.zeros_like(DB_fake_sample))\n",
        "                        d_B_loss = (d_B_loss_real + d_B_loss_fake) / 2\n",
        "                        d_loss = d_A_loss + d_B_loss\n",
        "\n",
        "                    # Calculate the gradients for generator and discriminator\n",
        "                    generator_A2B_gradients = gen_tape.gradient(target=g_A2B_loss,\n",
        "                                                                sources=self.generator_A2B.trainable_variables)\n",
        "                    generator_B2A_gradients = gen_tape.gradient(target=g_B2A_loss,\n",
        "                                                                sources=self.generator_B2A.trainable_variables)\n",
        "\n",
        "                    discriminator_A_gradients = disc_tape.gradient(target=d_A_loss,\n",
        "                                                                   sources=self.discriminator_A.trainable_variables)\n",
        "                    discriminator_B_gradients = disc_tape.gradient(target=d_B_loss,\n",
        "                                                                   sources=self.discriminator_B.trainable_variables)\n",
        "\n",
        "                    # Apply the gradients to the optimizer\n",
        "                    self.GA2B_optimizer.apply_gradients(zip(generator_A2B_gradients,\n",
        "                                                            self.generator_A2B.trainable_variables))\n",
        "                    self.GB2A_optimizer.apply_gradients(zip(generator_B2A_gradients,\n",
        "                                                            self.generator_B2A.trainable_variables))\n",
        "\n",
        "                    self.DA_optimizer.apply_gradients(zip(discriminator_A_gradients,\n",
        "                                                          self.discriminator_A.trainable_variables))\n",
        "                    self.DB_optimizer.apply_gradients(zip(discriminator_B_gradients,\n",
        "                                                          self.discriminator_B.trainable_variables))\n",
        "\n",
        "                    print('=================================================================')\n",
        "                    print((\"Epoch: [%2d] [%4d/%4d] time: %4.4f D_loss: %6.2f, G_loss: %6.2f, cycle_loss: %6.2f\" %\n",
        "                           (epoch, idx, batch_idxs, time.time() - start_time, d_loss, g_loss, cycle_loss)))\n",
        "\n",
        "                else:\n",
        "\n",
        "                    # To feed real_mixed\n",
        "                    batch_files_mixed = data_mixed[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "                    batch_samples_mixed = [np.load(batch_file) * 1. for batch_file in batch_files_mixed]\n",
        "                    real_mixed = np.array(batch_samples_mixed).astype(np.float32)\n",
        "\n",
        "                    with tf.GradientTape(persistent=True) as gen_tape, tf.GradientTape(persistent=True) as disc_tape:\n",
        "\n",
        "                        fake_B = self.generator_A2B(real_A,\n",
        "                                                    training=True)\n",
        "                        cycle_A = self.generator_B2A(fake_B,\n",
        "                                                     training=True)\n",
        "\n",
        "                        fake_A = self.generator_B2A(real_B,\n",
        "                                                    training=True)\n",
        "                        cycle_B = self.generator_A2B(fake_A,\n",
        "                                                     training=True)\n",
        "\n",
        "                        [fake_A_sample, fake_B_sample] = self.pool([fake_A, fake_B])\n",
        "\n",
        "                        DA_real = self.discriminator_A(real_A + gaussian_noise,\n",
        "                                                       training=True)\n",
        "                        DB_real = self.discriminator_B(real_B + gaussian_noise,\n",
        "                                                       training=True)\n",
        "\n",
        "                        DA_fake = self.discriminator_A(fake_A + gaussian_noise,\n",
        "                                                       training=True)\n",
        "                        DB_fake = self.discriminator_B(fake_B + gaussian_noise,\n",
        "                                                       training=True)\n",
        "\n",
        "                        DA_fake_sample = self.discriminator_A(fake_A_sample + gaussian_noise,\n",
        "                                                              training=True)\n",
        "                        DB_fake_sample = self.discriminator_B(fake_B_sample + gaussian_noise,\n",
        "                                                              training=True)\n",
        "\n",
        "                        DA_real_all = self.discriminator_A_all(real_mixed + gaussian_noise,\n",
        "                                                               training=True)\n",
        "                        DB_real_all = self.discriminator_B_all(real_mixed + gaussian_noise,\n",
        "                                                               training=True)\n",
        "\n",
        "                        DA_fake_sample_all = self.discriminator_A_all(fake_A_sample + gaussian_noise,\n",
        "                                                                      training=True)\n",
        "                        DB_fake_sample_all = self.discriminator_B_all(fake_B_sample + gaussian_noise,\n",
        "                                                                      training=True)\n",
        "\n",
        "                        # Generator loss\n",
        "                        cycle_loss = self.L1_lambda * (abs_criterion(real_A, cycle_A) + abs_criterion(real_B, cycle_B))\n",
        "                        g_A2B_loss = self.criterionGAN(DB_fake, tf.ones_like(DB_fake)) + cycle_loss\n",
        "                        g_B2A_loss = self.criterionGAN(DA_fake, tf.ones_like(DA_fake)) + cycle_loss\n",
        "                        g_loss = g_A2B_loss + g_B2A_loss - cycle_loss\n",
        "\n",
        "                        # Discriminator loss\n",
        "                        d_A_loss_real = self.criterionGAN(DA_real, tf.ones_like(DA_real))\n",
        "                        d_A_loss_fake = self.criterionGAN(DA_fake_sample, tf.zeros_like(DA_fake_sample))\n",
        "                        d_A_loss = (d_A_loss_real + d_A_loss_fake) / 2\n",
        "                        d_B_loss_real = self.criterionGAN(DB_real, tf.ones_like(DB_real))\n",
        "                        d_B_loss_fake = self.criterionGAN(DB_fake_sample, tf.zeros_like(DB_fake_sample))\n",
        "                        d_B_loss = (d_B_loss_real + d_B_loss_fake) / 2\n",
        "                        d_loss = d_A_loss + d_B_loss\n",
        "\n",
        "                        d_A_all_loss_real = self.criterionGAN(DA_real_all, tf.ones_like(DA_real_all))\n",
        "                        d_A_all_loss_fake = self.criterionGAN(DA_fake_sample_all, tf.zeros_like(DA_fake_sample_all))\n",
        "                        d_A_all_loss = (d_A_all_loss_real + d_A_all_loss_fake) / 2\n",
        "                        d_B_all_loss_real = self.criterionGAN(DB_real_all, tf.ones_like(DB_real_all))\n",
        "                        d_B_all_loss_fake = self.criterionGAN(DB_fake_sample_all, tf.zeros_like(DB_fake_sample_all))\n",
        "                        d_B_all_loss = (d_B_all_loss_real + d_B_all_loss_fake) / 2\n",
        "                        d_all_loss = d_A_all_loss + d_B_all_loss\n",
        "                        D_loss = d_loss + self.gamma * d_all_loss\n",
        "\n",
        "                    # Calculate the gradients for generator and discriminator\n",
        "                    generator_A2B_gradients = gen_tape.gradient(target=g_A2B_loss,\n",
        "                                                                sources=self.generator_A2B.trainable_variables)\n",
        "                    generator_B2A_gradients = gen_tape.gradient(target=g_B2A_loss,\n",
        "                                                                sources=self.generator_B2A.trainable_variables)\n",
        "\n",
        "                    discriminator_A_gradients = disc_tape.gradient(target=d_A_loss,\n",
        "                                                                   sources=self.discriminator_A.trainable_variables)\n",
        "                    discriminator_B_gradients = disc_tape.gradient(target=d_B_loss,\n",
        "                                                                   sources=self.discriminator_B.trainable_variables)\n",
        "\n",
        "                    discriminator_A_all_gradients = disc_tape.gradient(target=d_A_all_loss,\n",
        "                                                                   sources=self.discriminator_A_all.trainable_variables)\n",
        "                    discriminator_B_all_gradients = disc_tape.gradient(target=d_B_all_loss,\n",
        "                                                                   sources=self.discriminator_B_all.trainable_variables)\n",
        "\n",
        "                    # Apply the gradients to the optimizer\n",
        "                    self.GA2B_optimizer.apply_gradients(zip(generator_A2B_gradients,\n",
        "                                                            self.generator_A2B.trainable_variables))\n",
        "                    self.GB2A_optimizer.apply_gradients(zip(generator_B2A_gradients,\n",
        "                                                            self.generator_B2A.trainable_variables))\n",
        "\n",
        "                    self.DA_optimizer.apply_gradients(zip(discriminator_A_gradients,\n",
        "                                                          self.discriminator_A.trainable_variables))\n",
        "                    self.DB_optimizer.apply_gradients(zip(discriminator_B_gradients,\n",
        "                                                          self.discriminator_B.trainable_variables))\n",
        "\n",
        "                    self.DA_all_optimizer.apply_gradients(zip(discriminator_A_all_gradients,\n",
        "                                                              self.discriminator_A_all.trainable_variables))\n",
        "                    self.DB_all_optimizer.apply_gradients(zip(discriminator_B_all_gradients,\n",
        "                                                              self.discriminator_B_all.trainable_variables))\n",
        "\n",
        "                    print('=================================================================')\n",
        "                    print((\"Epoch: [%2d] [%4d/%4d] time: %4.4f D_loss: %6.2f, G_loss: %6.2f\" %\n",
        "                           (epoch, idx, batch_idxs, time.time() - start_time, D_loss, g_loss)))\n",
        "\n",
        "                counter += 1\n",
        "\n",
        "                # generate samples during training to track the learning process\n",
        "                if np.mod(counter, args.print_freq) == 1:\n",
        "                    sample_dir = os.path.join(self.sample_dir,\n",
        "                                              '{}2{}_{}_{}_{}'.format(self.dataset_A_dir,\n",
        "                                                                      self.dataset_B_dir,\n",
        "                                                                      self.now_datetime,\n",
        "                                                                      self.model,\n",
        "                                                                      self.sigma_d))\n",
        "                    if not os.path.exists(sample_dir):\n",
        "                        os.makedirs(sample_dir)\n",
        "\n",
        "                    # to binary, 0 denotes note off, 1 denotes note on\n",
        "                    samples = [to_binary(real_A, 0.5),\n",
        "                               to_binary(fake_B, 0.5),\n",
        "                               to_binary(cycle_A, 0.5),\n",
        "                               to_binary(real_B, 0.5),\n",
        "                               to_binary(fake_A, 0.5),\n",
        "                               to_binary(cycle_B, 0.5)]\n",
        "\n",
        "                    self.sample_model(samples=samples,\n",
        "                                      sample_dir=sample_dir,\n",
        "                                      epoch=epoch,\n",
        "                                      idx=idx)\n",
        "\n",
        "                if np.mod(counter, args.save_freq) == 1:\n",
        "                    self.checkpoint_manager.save(counter)\n",
        "\n",
        "    def sample_model(self, samples, sample_dir, epoch, idx):\n",
        "\n",
        "        print('generating samples during learning......')\n",
        "\n",
        "        if not os.path.exists(os.path.join(sample_dir, 'B2A')):\n",
        "            os.makedirs(os.path.join(sample_dir, 'B2A'))\n",
        "        if not os.path.exists(os.path.join(sample_dir, 'A2B')):\n",
        "            os.makedirs(os.path.join(sample_dir, 'A2B'))\n",
        "\n",
        "        save_midis(samples[0], './{}/A2B/{:02d}_{:04d}_origin.mid'.format(sample_dir, epoch, idx))\n",
        "        save_midis(samples[1], './{}/A2B/{:02d}_{:04d}_transfer.mid'.format(sample_dir, epoch, idx))\n",
        "        save_midis(samples[2], './{}/A2B/{:02d}_{:04d}_cycle.mid'.format(sample_dir, epoch, idx))\n",
        "        save_midis(samples[3], './{}/B2A/{:02d}_{:04d}_origin.mid'.format(sample_dir, epoch, idx))\n",
        "        save_midis(samples[4], './{}/B2A/{:02d}_{:04d}_transfer.mid'.format(sample_dir, epoch, idx))\n",
        "        save_midis(samples[5], './{}/B2A/{:02d}_{:04d}_cycle.mid'.format(sample_dir, epoch, idx))\n",
        "\n",
        "    def test(self, args):\n",
        "\n",
        "        if args.which_direction == 'AtoB':\n",
        "            sample_files = glob('./datasets/{}/test/*.*'.format(self.dataset_A_dir))\n",
        "        elif args.which_direction == 'BtoA':\n",
        "            sample_files = glob('./datasets/{}/test/*.*'.format(self.dataset_B_dir))\n",
        "        else:\n",
        "            raise Exception('--which_direction must be AtoB or BtoA')\n",
        "        sample_files.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0].split('_')[-1]))\n",
        "\n",
        "        if self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint):\n",
        "            print(\" [*] Load checkpoint succeeded!\")\n",
        "        else:\n",
        "            print(\" [!] Load checkpoint failed...\")\n",
        "\n",
        "        test_dir_mid = os.path.join(args.test_dir, '{}2{}_{}_{}_{}/{}/mid'.format(self.dataset_A_dir,\n",
        "                                                                                  self.dataset_B_dir,\n",
        "                                                                                  self.now_datetime,\n",
        "                                                                                  self.model,\n",
        "                                                                                  self.sigma_d,\n",
        "                                                                                  args.which_direction))\n",
        "        if not os.path.exists(test_dir_mid):\n",
        "            os.makedirs(test_dir_mid)\n",
        "\n",
        "        test_dir_npy = os.path.join(args.test_dir, '{}2{}_{}_{}_{}/{}/npy'.format(self.dataset_A_dir,\n",
        "                                                                                  self.dataset_B_dir,\n",
        "                                                                                  self.now_datetime,\n",
        "                                                                                  self.model,\n",
        "                                                                                  self.sigma_d,\n",
        "                                                                                  args.which_direction))\n",
        "        if not os.path.exists(test_dir_npy):\n",
        "            os.makedirs(test_dir_npy)\n",
        "\n",
        "        for idx in range(len(sample_files)):\n",
        "            print('Processing midi: ', sample_files[idx])\n",
        "            sample_npy = np.load(sample_files[idx]) * 1.\n",
        "\n",
        "            # save midis\n",
        "            origin = sample_npy.reshape(1, sample_npy.shape[0], sample_npy.shape[1], 1)\n",
        "            midi_path_origin = os.path.join(test_dir_mid, '{}_origin.mid'.format(idx + 1))\n",
        "            midi_path_transfer = os.path.join(test_dir_mid, '{}_transfer.mid'.format(idx + 1))\n",
        "            midi_path_cycle = os.path.join(test_dir_mid, '{}_cycle.mid'.format(idx + 1))\n",
        "\n",
        "            if args.which_direction == 'AtoB':\n",
        "\n",
        "                transfer = self.generator_A2B(origin,\n",
        "                                              training=False)\n",
        "                cycle = self.generator_B2A(transfer,\n",
        "                                           training=False)\n",
        "\n",
        "            else:\n",
        "\n",
        "                transfer = self.generator_B2A(origin,\n",
        "                                              training=False)\n",
        "                cycle = self.generator_A2B(transfer,\n",
        "                                           training=False)\n",
        "\n",
        "            save_midis(origin, midi_path_origin)\n",
        "            save_midis(transfer, midi_path_transfer)\n",
        "            save_midis(cycle, midi_path_cycle)\n",
        "\n",
        "            # save npy files\n",
        "            npy_path_origin = os.path.join(test_dir_npy, 'origin')\n",
        "            npy_path_transfer = os.path.join(test_dir_npy, 'transfer')\n",
        "            npy_path_cycle = os.path.join(test_dir_npy, 'cycle')\n",
        "\n",
        "            if not os.path.exists(npy_path_origin):\n",
        "                os.makedirs(npy_path_origin)\n",
        "            if not os.path.exists(npy_path_transfer):\n",
        "                os.makedirs(npy_path_transfer)\n",
        "            if not os.path.exists(npy_path_cycle):\n",
        "                os.makedirs(npy_path_cycle)\n",
        "\n",
        "            np.save(os.path.join(npy_path_origin, '{}_origin.npy'.format(idx + 1)), origin)\n",
        "            np.save(os.path.join(npy_path_transfer, '{}_transfer.npy'.format(idx + 1)), transfer)\n",
        "            np.save(os.path.join(npy_path_cycle, '{}_cycle.npy'.format(idx + 1)), cycle)\n",
        "\n",
        "    def inference(self, sample_npys, result_path):\n",
        "        if self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint):\n",
        "            print(\" [*] Load checkpoint succeeded!\")\n",
        "        else:\n",
        "            print(\" [!] Load checkpoint failed...\")\n",
        "\n",
        "        results = []\n",
        "        for idx in range(len(sample_npys)):\n",
        "            sample_npy = sample_npys[idx] * 1.\n",
        "            \n",
        "            origin = sample_npy.reshape(1, sample_npy.shape[0], sample_npy.shape[1], 1)\n",
        "\n",
        "            if args.which_direction == 'AtoB':\n",
        "                transfer = self.generator_A2B(origin,\n",
        "                                              training=False)\n",
        "            else:\n",
        "                transfer = self.generator_B2A(origin,\n",
        "                                              training=False)\n",
        "            results += [transfer]\n",
        "            # save_midis(transfer, result_path, 127)\n",
        "        \n",
        "        result = results[0]\n",
        "        for i in range(1, len(results)):\n",
        "          result = np.concatenate((result, results[i]), axis=0)\n",
        "        save_midis(result, result_path)\n",
        "        return result\n",
        "\n",
        "    def test_famous(self, args):\n",
        "\n",
        "        song = np.load('./datasets/famous_songs/P2C/merged_npy/YMCA.npy')\n",
        "\n",
        "        if self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint):\n",
        "            print(\" [*] Load checkpoint succeeded!\")\n",
        "        else:\n",
        "            print(\" [!] Load checkpoint failed...\")\n",
        "\n",
        "        if args.which_direction == 'AtoB':\n",
        "            transfer = self.generator_A2B(song,\n",
        "                                          training=False)\n",
        "        else:\n",
        "            transfer = self.generator_B2A(song,\n",
        "                                          training=False)\n",
        "\n",
        "        save_midis(transfer, './datasets/famous_songs/P2C/transfer/YMCA.mid', 127)\n",
        "        np.save('./datasets/famous_songs/P2C/transfer/YMCA.npy', transfer)\n",
        "\n",
        "LAST_BAR_MODE = 'remove'\n",
        "\n",
        "def process_midi(midi_path):\n",
        "\n",
        "    multitrack = Multitrack(beat_resolution=4)\n",
        "    x = pretty_midi.PrettyMIDI(midi_path)\n",
        "    multitrack.parse_pretty_midi(x)\n",
        "\n",
        "    category_list = {'Piano': [], 'Drums': []}\n",
        "    program_dict = {'Piano': 0, 'Drums': 0}\n",
        "\n",
        "    for idx, track in enumerate(multitrack.tracks):\n",
        "        if track.is_drum:\n",
        "            category_list['Drums'].append(idx)\n",
        "        else:\n",
        "            category_list['Piano'].append(idx)\n",
        "    tracks = []\n",
        "    merged = multitrack[category_list['Piano']].get_merged_pianoroll()\n",
        "\n",
        "    pr = get_bar_piano_roll(merged)\n",
        "    pr_clip = pr[:, :, 24:108]\n",
        "    if int(pr_clip.shape[0] % 4) != 0:\n",
        "        pad = np.zeros(pr_clip.shape)[:4 - pr_clip.shape[0] % 4, :, :]\n",
        "        pr_clip = np.concatenate((pr_clip, pad), axis=0)\n",
        "    pr_re = pr_clip.reshape(-1, 64, 84, 1)\n",
        "\n",
        "    train = pr_re\n",
        "    x = (train > 0.0)\n",
        "    result = []\n",
        "    for i in range(x.shape[0]):\n",
        "      if np.max(x[i]):\n",
        "          result += [x[i]]\n",
        "    return result\n",
        "\n",
        "class Args:\n",
        "  dataset_A_dir='CP_C'\n",
        "  dataset_B_dir='CP_P'\n",
        "  epoch=10\n",
        "  epoch_step=10\n",
        "  batch_size=4\n",
        "  time_step=64\n",
        "  pitch_range=84\n",
        "  ngf=64\n",
        "  ndf=64\n",
        "  input_nc=1\n",
        "  output_nc=1\n",
        "  lr=0.0002\n",
        "  beta1=0.5\n",
        "  which_direction='AtoB'\n",
        "  phase='train'\n",
        "  save_freq=1000\n",
        "  print_freq=100\n",
        "  continue_train=False\n",
        "  checkpoint_dir='./checkpoint'\n",
        "  model_dir=''\n",
        "  sample_dir='./samples'\n",
        "  test_dir='./test'\n",
        "  log_dir='./log'\n",
        "  L1_lambda=10.0\n",
        "  gamma=1.0\n",
        "  max_size=50\n",
        "  sigma_c=0.01\n",
        "  sigma_d=0.01\n",
        "  model='full'\n",
        "  type='classifier'\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    self.__dict__.update(kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m1FGdpq0EOv",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Upload MIDI file for inference\n",
        "uploaded = files.upload()\n",
        "file_names = list(uploaded.keys())\n",
        "filename = file_names[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMhF_gtl051E",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Perform Inference\n",
        "\n",
        "direction = 'BtoA' #@param [\"AtoB\", \"BtoA\"]\n",
        "\n",
        "model_name = {\n",
        "    'Classic <-> Pop': 'CP_C2CP_P_2020-08-31_base_0.1',\n",
        "    'Jazz <-> Pop': 'JP_J2JP_P_2020-08-28_base_0.01'\n",
        "    }[model_type]\n",
        "\n",
        "output_path = \"../result.mid\"\n",
        "args = Args(type='cyclegan',\n",
        "            model='base',\n",
        "            phase='test',\n",
        "            checkpoint_dir='models',\n",
        "            model_dir=model_name,\n",
        "            which_direction=direction)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  model = CycleGAN(args)\n",
        "  model.inference(process_midi(filename), output_path)\n",
        "print(\"Inference finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sXlqmlw08aa",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Download Result\n",
        "files.download(output_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}